{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Coding Homework 9: [put your name here]\n",
    "\n",
    "Go through this notebook, following the instructions! \n",
    "\n",
    "- You can add new cells if you need (with the \"+\" button above); but, deleting cells could very likely cause your notebook to fail MarkUs autotesting (and you'd have to start over and re-enter your answers into a completely fresh version of the notebook to get things to work again...)\n",
    "\n",
    "> TAs will mark this assignment by checking ***MarkUs*** autotests and then checking for the presence of decision tree visualization code in `Q7` and then manually grading `Q12`.\n",
    "> - The following questions \"automatically fail\" during automated testing so that MarkUs exposes example answers for student review and consideration for these problems.  These \"failed MarkUs tests\" are not counted against the student: `Q4`, `Q5`, and `Q10`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We begin by importing dataset and the libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Imports",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import graphviz as gv\n",
    "happiness2017 = pd.read_csv(\"happiness2017.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Gallup Report Happiness Survey\n",
    "Using data from the Gallup World Poll (and the World Happiness Report), we are interested in predicting which factors influence life expectancy around the world. These data are in the file `\"happinessdata_2017.csv\"`, which we imported as `happiness2017`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Q0: Add a new column to `happiness2017` called `life_exp_good` which is `True` for countries with life expectancy (strictly) higher than 65 years, and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q0: your answer will be tested based on your `happiness2017` object! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Q1: Create a new dataframe `happiness2017_cleaned` containing the columns below and all rows with `NaN` entries dropped, and an 80/20 split (80% training set and 20% testing set) for this new data.\n",
    "\n",
    "- `life_exp_good`, `logGDP`, `social_support`, `freedom`, and `generosity`\n",
    "\n",
    "> Only remove rows with `NaN` entries once you've subset to the columns you're interested in. This will minimize potentially unnecessary data loss... Of course we might want to consider imputing missing data to further mitigate data loss, but the considerations for doing so are more advanced than the level of our course, so we'll not consider that for now. At any rate, `NaN` entries can't be used in their raw form with the `scikit-learn` methodologies below, so we do need to remove them to proceed with our analyses.\n",
    "> \n",
    "> To perform the train/test split in a reproducible way, we're going to set a \"random seed\"; and, in preparation for this, let's take a moment to motivate our choice of $1985$ for the \"random seed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# 19, 19, 1985!\n",
    "YouTubeVideo('K38xNqZvBJI', width=800, height=500)\n",
    "# Remember, always choose your favorite number for your \"random seeds\"\n",
    "# The specific number you choose doesn't even really matter, which is why \n",
    "# it's so important to make a big deal about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Data_Split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: your answer will be tested based on your `happiness2017_cleaned` and `train` and `test` objects! \n",
    "np.random.seed(1985) # Do NOT change this line: it sets the \"random number generation seed\"\n",
    "# which makes the \"pseudorandomness\" gererated in the code the same every time and this\n",
    "# makes the code reproducibile which ensures that our testing code works properly every time\n",
    "happiness2017_cleaned = None\n",
    "train, test = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Q2: Train a classification tree `clf` using only the  `social_support` variable to predict if a country has good life expectancy\n",
    "\n",
    "#### Use default values for all (tuning) parameters instantiating the Decision Tree Classifier.\n",
    "\n",
    "> Hint: should you use the `train` data, or the `test` data, or all this data combined to fit the classification tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: your `clf` object will be tested for correctness (and you don't need to do anything else here yet)!\n",
    "np.random.seed(1985) # Do not change this line as there is some randomeness when calling `clf.fit(...)`\n",
    "clf = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Now you can visualize your tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree.plot_tree(clf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Or to make it more immediately readible we can use graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=[\"social_support\"],\n",
    "                                class_names=[\"Bad\",\"Good\"],\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = gv.Source(dot_data, format='SVG')\n",
    "graph # Comment this line and re-run the cell so the figure doesn't render if your final file size is too large for MarkUs\n",
    "# Same solution if MarkUs notebook renderer gives you an error of \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "And to limit the visualization itself we can add the `max_depth` parameter to our call of `export_graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, max_depth=3,\n",
    "                                feature_names=[\"social_support\"],\n",
    "                                class_names=[\"Bad\",\"Good\"],\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = gv.Source(dot_data, format='SVG')\n",
    "graph # Comment this line and re-run the cell so the figure doesn't render if MarkUs notebook renderer gives you an error of \n",
    "# \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Q3: How many observation are in the training data set and the test data set?\n",
    "\n",
    "> - Hint: a single observation consists of all the measurements made on a single entity. In Machine Learning, the  \"vector\" of all values measured for a single entity comprise a single \"observation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q3_empty_cell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: your answer will be tested!\n",
    "num_train = None # Replace this with the number of observations in the training set\n",
    "num_test = None # Replace this with the number of observations in the test set\n",
    "Q3 = (num_train, num_test) # Do not change this line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Q4: Why did you fit the classification tree with the data set you did?\n",
    "\n",
    "#### Write a one to two sentence answer to this question in markdown cell below\n",
    "- Compare your response to the answer given in the ***MarkUs*** output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "> Answer here...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Q5: Comment on the complexity of the decision tree classification model visualized above, especially in light of the fact that only a single feature was used to predict the outcome in this model.\n",
    "\n",
    "\n",
    "#### Write a one to two sentence answer to this question in markdown cell below\n",
    "- Compare your response to the answer given in the ***MarkUs*** output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "> Answer here...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Q6: Use the `clf.predict()` method to answer the following questions and confirm your answer using the first `graphviz` visualization of the decision tree\n",
    "\n",
    "#### Use values of `False`  and `True` as given by `clf.predict()` (corresponding to \"Bad\" and \"Good\"): always be sure you know what binary/boolean predictions actually correspond to in the original terms of a problem\n",
    "\n",
    "a) Does your decision tree predict that a country with `social_support = 0.49` has good life expectancy?  \n",
    "b) what if `social_support = 0.5`  \n",
    "c) what if `social_support = 0.51`  \n",
    "d) what if `social_support = 0.9`  \n",
    "\n",
    "> - Hint: `pd.DataFrame({'name': [<values>]})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066e9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: your answer will be tested!\n",
    "Q6a = None # Replace this with True or False: don't supply something like array([False])\n",
    "Q6b = None\n",
    "Q6c = None\n",
    "Q6d = None\n",
    "Q6 = (Q6a, Q6b, Q6c, Q6d) # Do not change this line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Do these predictions make sense to you?\n",
    "\n",
    "Look at how these small differences in the input rapidly change the predicted label... it seems kind of strange. It's a little hard to intuitively see why predictions should change like this... it makes you wonder if the model is really doing anything meaningful here.  \n",
    "\n",
    "Perhaps the model might just actually be overly complex and convoluted and might be overinterpreting the data used to fit it (which we call overfitting). Since you'll probably agree that the behaviour of the model that we're observing seems a bit off, you'll probably also agree that it's a reasonable idea to try reduce the complexity of the model so it can be more reliably estimated with the data at hand.  With that in mind, create and fit a new classification tree `clf2` on the same inputs with a maximum depth of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clf2_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1985) # Do not change this line as there is some randomeness when calling `clf.fit(...)`\n",
    "clf2 = None # The parameter that you're looking for is `max_depth`\n",
    "# But note that this is fundamentally different than the `max_depth` parameter for `tree.export_graphviz` (why?)\n",
    "\n",
    "# Uncomment and run the line below to see the function documentation\n",
    "#tree.DecisionTreeClassifier?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Q7: Visualize the  `clf2` Decision Tree and reanswer the questions asked in Q6 with `clf2`.\n",
    "\n",
    "#### Use the same train/test split data used so far when fitting  `clf2` above.\n",
    "\n",
    "> - Hint: Make sure you're now using `clf2` rather than `clf` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50849fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your Decision Tree visualization code here: your code is what will be checked here\n",
    "\n",
    "graph # Comment this line and re-run the cell so the figure doesn't render if MarkUs notebook renderer gives you an error of \n",
    "# \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6775aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: your answer will be tested!\n",
    "Q7a = None # Replace this with True or False\n",
    "Q7b = None\n",
    "Q7c = None\n",
    "Q7d = None\n",
    "Q7 = (Q7a, Q7b, Q7c, Q7d) # Do not change this line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Train classification tree `clf3` with features `logGDP`, `social_support`, `freedom`, and `generosity` (to again predict if a country has good life expectancy)\n",
    "\n",
    "#### Use the same train/test split data used so far and use default (tuning) parameters (no maximum tree depth) when instantiating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clf3_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1985) # Do not change this line as there is some randomeness when calling `clf.fit(...)`\n",
    "clf3 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf3, out_file=None, max_depth=3,\n",
    "                                feature_names=[\"logGDP\", \"social_support\", \"freedom\", \"generosity\"],\n",
    "                                class_names=[\"Bad\", \"Good\"],\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = gv.Source(dot_data, format='SVG')\n",
    "graph # You can comment this line and re-run so the figure doesn't render if MarkUs notebook renderer gives you an error of \n",
    "# \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Q8: Use the testing dataset you created in Q1 to create confusion matrices for `clf2` and `clf3`. Report the sensitivity (true positive rate), specificity (true negative rate) and accuracy for each of the trees/models.\n",
    "\n",
    "#### Provide your answers as decimal numbers with three signifiant digits, such as `0.123` (and not as percentages like `12.3%`), and treat “Good” life expectancy as the positive response and prediction class. \n",
    "\n",
    "> - Hint 0: Use `np.round(0.1234,3)` to produce the correct rouding for the answers  \n",
    "> - Hint 1: Does the `y_true` or `y_pred` parameter go first in the `confusion_matrix` function?  \n",
    "> - Hint 2: Which columns/features of the `test` data set should be used for `clf2` versus `clf3`?  \n",
    "> - Hint 3: Making sure you know how a confusion matrix is labeled can be tricky; so, <br>to keep things\n",
    ">   easier for you the way it works is demonstrated below \n",
    ">   ```python\n",
    ">      ConfusionMatrixDisplay(confusion_matrix(..., labels=[False, True]), \n",
    ">                             display_labels=[\"Bad\",\"Good\"]).plot()\n",
    ">   ```\n",
    ">   (i.e., labels are sorted so `False` comes before `True` and this corresponds to \"Bad\" then \"Good\")\n",
    "> - Hint 4: If you need more help, see the [Confusion Matrices and Metrics](#cf) section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q8_empty_cell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: your answer will be tested!\n",
    "(clf2_sensitivity, clf2_specificity, clf2_accuracy) = (None, None, None) # Replace the `None`s with the corresponding answers\n",
    "(clf3_sensitivity, clf3_specificity, clf3_accuracy) = (None, None, None)\n",
    "Q8 = (clf2_sensitivity, clf2_specificity, clf2_accuracy) + (clf3_sensitivity, clf3_specificity, clf3_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Compared to understanding the contribution of different covariates towards the final predicted values of multiple linear regression models (where you can just read off the equation to see how predictions work), the extent to which we do not understand the overall contributions of the different features to the final predictions from our decision trees should feel a bit off-putting. To remedy this we can use so-called **Feature Importance** heuristics to judge how relatively important the different features are in the final decision tree predictions. There are a number of ways to do this (such as counting the number of nodes which split on each feature divided by the total number of splits in the tree), but as seen in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), a fit `DecisionTreeClassifier` model has a `.feature_importances_` attribute which measures the relative  contribution of each feature to the explanatory power of the model.  \n",
    "\n",
    "> The way a decision tree is fit is that at each step in the construction process of adding a new decision node splitting rule to the current tree structure. All possible decision rules for all possible variables are considered, and the one that improves the prediction the most (as measured by the criterion of either \"Gini impurity\" or \"Shannon entropy\") legally and sufficiently according to the tuning parameters rules of the decision tree is added to the decision tree.  The overall \"criterion\" noted above improves with each new decision node splitting rule, so the improvement can thus be tracked and the contributions attributed to the feature upon which the decision node splitting rule is based.  This means the relative contribution of each feature to the overall explanatory power of the model can be calculated, and this is what the `.feature_importances_` attribute does. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Q9: Which predictor variable is most important for making predictions according to clf3?\n",
    "\n",
    "> - Hint 0: The values in the `.feature_importances_` attribute correspond to values in the `.feature_names_in_` attribute\n",
    "> - Hint 1: The usual way to visualize Feature Importances would be as\n",
    "    ```python\n",
    "    import plotly.express as px\n",
    "    px.bar(pd.DataFrame({\"Feature Importances\": clf3.feature_importances_, \n",
    "                         \"Feature names\": clf3.feature_names_in_}), \n",
    "           x=\"Feature Importances\", y=\"Feature names\")    \n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q9_empty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9: your answer will be tested!\n",
    "Q9 = None # Replace this with the (str) name of the most relevant predictor variable\n",
    "# E.g., Q9 = \"A Variable Name\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Q10: Describe the differences and similarities of interpreting coefficients in linear model regression versus feature importances in decision trees.\n",
    "#### Write a 1-2 sentence answer to this question in markdown cell below\n",
    "- Compare your response to the answer given in the ***MarkUs*** output.\n",
    "\n",
    "> - Hint: Consider the differing goals of linear regression and classification trees."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "> Answer here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "<a id='cf'></a>\n",
    "# Confusion Matrices and Metrics\n",
    "\n",
    "- **Accuracy** is the proportion of cases that are correctly identified.\n",
    "- **Sensitivity** is the proportion of actual positive cases which are correctly identified to be positive (as true positives)\n",
    "    - **Sensitivity** is also known as **true positive rate (TPR)**\n",
    "- **Specificity** is the proportion of actual negative cases which are correctly identified to be negative (as true negative)\n",
    "    - **Specificity** is also known as **true negative rate (TNR)**\n",
    "- **False positive rates (FPR)** are defined to be the proportion of actually negative cases which are incorrectly identified (as false positives)\n",
    "- **False negative rates (FNR)** are defined to be the proportion of actually positive cases which are incorrectly identified (as false negatives)\n",
    "    - *but noticed how the FPR and FNR work in a sort of \"flipped\" manner in these definitions as they are defined with respect to the truth*\n",
    "\n",
    "In formulas\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy}  & = {} (TP+TN)/\\text{\"total # of cases\"}\\\\\n",
    "TPR & = {} TP/(TP+FN) = 1-FNR \\\\\n",
    "TNR & = {} TN/(TN+FP) = 1-FPR\n",
    "\\end{align*}\n",
    "\n",
    "and you can read more and see a (greatly expanded) handy list of formulas at the following [wikipedia page.](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Two classification trees were built to predict which individuals have a disease using different sets of potential predictors. We use each of these trees to predict disease status for 100 new individuals. Below are confusion matrices corresponding to these two classification trees. The columns are the actual outcome, the rows are predicted outcomes (which is different than how the `ConfusionMatrixDisplay(confusion_matrix(...` functions worked about... you always have to make sure you know what the labels on your confusion matrices are!!)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Tree A**         | Disease | No disease | $\\hspace{1in}$ | **Tree B**         | Disease | No disease |\n",
    "|--------------------|---------|------------|----------------|--------------------|---------|------------|\n",
    "| Predict disease    | 36      | 22         |                | Predict disease    | 24      | 6          |\n",
    "| Predict no disease | 2       | 40         |                | Predict no disease | 14      | 56         |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Q11: Calculate the accuracy, false-positive rate, and false negative rate for each classification tree.\n",
    "Here, a “positive” result means we predict an individual has the disease and a “negative” result means we predict they do not.\n",
    "Round each value to 2 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11: your answer will be tested!\n",
    "TreeA_accuracy = None\n",
    "TreeA_false_positive_rate = None\n",
    "TreeA_false_negative_rate = None\n",
    "\n",
    "TreeB_accuracy = None\n",
    "TreeB_false_positive_rate = None\n",
    "TreeB_false_negative_rate = None\n",
    "\n",
    "Q11 = (TreeA_accuracy, TreeA_false_positive_rate, TreeA_false_negative_rate, TreeB_accuracy, TreeB_false_positive_rate, TreeB_false_negative_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Q12: Which tree would you prefer to put into use in predicting if individuals are ill?\n",
    "#### Write a 1-3 sentence answer to this question in markdown cell below.\n",
    "\n",
    "> - Hint 1: Make reference to the metrics you calculated in Q11, and any others you think might matter.\n",
    "> - Hint 2: Interpret what the metrics mean in the context of the problem before deciding how much the metrics matter to you.\n",
    "> - Hint 3: Indicate what tradeoffs might you find acceptable and unacceptable.\n",
    "\n",
    "- This question will be manually graded by TAs. They are looking for a well-written and reasonably sensible answer as opposed to a specific answer.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "> Answer here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Geometric Interpretation of Decision Tree Predictions\n",
    "Data was collected on 30 cancer patients to investigate the effectiveness (Yes/No) of a treatment. Two quantitative variables x1 and x2 (taking values between 0 and 1) are thought to be important predictors of effectiveness. Suppose that the rectangles labeled as nodes in the scatter plot below represent nodes of a classification tree.\n",
    "![Scatter plot with a horizontal x1 axis and vertical x2 axis, both ranging from 0.00 to 1.00, and blue triangular points representing Effectiveness = 'Yes' and round orange points representing Effectiveness = 'No'. It is divided into 4 regions, labelled nodes 1-4. Node 1 is the bottom left region, node 4 the bottom right, node 3 the top left, and node 2 the top right. The top regions are divided from the bottom ones by a horizontal line along x_2=0.50. Node 2 is separated from node 3 by a line at x1=0.50. Node 2 is separated from node 3 by a line at x1=0.50. Node 1 has 5 'Yes' nodes and 7 'No' Nodes. Node 2 has 3 'Yes' nodes and 2 'No' Nodes. Node 3 has 1 'Yes' node and 3 'No' Nodes. Node 4 has 2 'Yes' nodes and 7 'No' Nodes.](im/9/HW9_Q7_Graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Q13: The diagram above is the geometric interpretation of a classification tree to predict drug effectiveness based on two predictors, x1 and x2. What is the predicted class of each node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q13: your answer will be tested!\n",
    "Q13_Node1 = None # Replace with 'Yes' or 'No'\n",
    "Q13_Node2 = None\n",
    "Q13_Node3 = None\n",
    "Q13_Node4 = None\n",
    "Q13 = (Q13_Node1, Q13_Node2, Q13_Node3, Q13_Node4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Q14: What is the first variable that the decision tree splits on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14: your answer will be tested!\n",
    "Q14 = None # Replace with 'x1' or 'x2'"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
