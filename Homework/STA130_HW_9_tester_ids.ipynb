{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Coding Homework 9: [put your name here]\n",
    "\n",
    "Go through this notebook, following the instructions! \n",
    "\n",
    "- You can add new cells if you need (with the \"+\" button above); but, deleting cells could very likely cause your notebook to fail MarkUs autotesting (and you'd have to start over and re-enter your answers into a completely fresh version of the notebook to get things to work again...)\n",
    "\n",
    "> TAs will mark this assignment by checking ***MarkUs*** autotests and then checking for the presence of decision tree visualization code in `Q7` and then manually grading `Q12`.\n",
    "> - The following questions \"automatically fail\" during automated testing so that MarkUs exposes example answers for student review and consideration for these problems.  These \"failed MarkUs tests\" are not counted against the student: `Q4`, `Q5`, and `Q10`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We begin by importing dataset and the libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'happiness2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgraphviz\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgv\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m happiness2017 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mhappiness2017.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'happiness2017.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import graphviz as gv\n",
    "happiness2017 = pd.read_csv(\"happiness2017.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Gallup Report Happiness Survey\n",
    "Using data from the Gallup World Poll (and the World Happiness Report), we are interested in predicting which factors influence life expectancy around the world. These data are in the file happinessdata_2017.csv, which we imported as `happiness2017`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Q0: Add a new column to `happiness2017` called `life_exp_good` which is `True` for countries with life expectancy (strictly) higher than 65 years, and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q0: your answer will be tested based on your `happiness2017` object! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8511a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sum(happiness2017.life_exp_good) == 573\n",
    "hint = \"Use .head() to see the existing columns and then use a boolean condition to define your new column.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q0\n",
    "\n",
    "assert test, hint+\"\\n\\nState Check: \"+str(sum(happiness2017.life_exp_good))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Q1: Create a new dataframe `happiness2017_cleaned` containing the columns below and all rows with `NaN` entries dropped, and an 80/20 split (80% training set and 20% testing set) for this new data.\n",
    "\n",
    "- `life_exp_good`, `logGDP`, `social_support`, `freedom`, and `generosity`\n",
    "\n",
    "> To do this in a reproducible way, we're going to set a \"random seed\"; and, in preparation for this, let's take a moment to motivate our choice of $1985$ for the \"random seed\".\n",
    ">\n",
    "> Only remove rows with `NaN` entries once you've subset to the columns you're interested in. This will minimize potentially unnecessary data loss... Of course we might want to consider imputing missing data to further mitigate data loss, but the considerations for doing so are more advanced than the level of our course, so we'll not consider that for now. At any rate, `NaN` entries can't be used in their raw form with the `scikit-learn` methodologies below, so we do need to remove them to proceed with our analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# 19, 19, 1985!\n",
    "YouTubeVideo('K38xNqZvBJI', width=800, height=500)\n",
    "# Remember, always choose your favorite number for your \"random seeds\"\n",
    "# The specific number you choose doesn't even really matter, which is why \n",
    "# it's so important to make a big deal about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Data_Split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: your answer will be tested based on your `happiness2017_cleaned` and `train` and `test` objects! \n",
    "np.random.seed(1985) # Do NOT change this line: it sets the \"random number generation seed\"\n",
    "# which makes the \"pseudorandomness\" gererated in the code the same every time and this\n",
    "# makes the code reproducibile which ensures that our testing code works properly every time\n",
    "happiness2017_cleaned = None\n",
    "train, test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e84bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"happiness2017[['life_exp_good', 'logGDP', 'social_support', 'freedom', 'generosity']].dropna(axis=0)? And train, test = model_selection.train_test_split(happiness2017_cleaned, train_size=0.8)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9b788c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'happiness2017_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test_Q1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhappiness2017_cleaned\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m happiness2017_cleaned\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m==\u001b[39m(\u001b[38;5;241m1298\u001b[39m, \u001b[38;5;241m5\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m train\u001b[38;5;241m.\u001b[39mlife_exp_good\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m408\u001b[39m, hint\n",
      "\u001b[0;31mNameError\u001b[0m: name 'happiness2017_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# test_Q1\n",
    "assert happiness2017_cleaned.shape==(1298, 5) and train.life_exp_good.sum()==408, hint+\"\\n\\nState Check: \"+str(happiness2017_cleaned.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Q2: Train a classification tree `clf` using only the  `social_support` variable to predict if a country has good life expectancy\n",
    "\n",
    "#### Use default values for all (tuning) parameters instantiating the Decision Tree Classifier.\n",
    "\n",
    "> Hint: should you use the `train` data, or the `test` data, or all this data combined to fit the classification tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: your `clf` object will be tested for correctness (and you don't need to do anything else here yet)!\n",
    "np.random.seed(1985) # Do not change this line as there is some randomeness when calling `clf.fit(...)`\n",
    "clf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q2_preparation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Why wouldn't `clf.fit(train.iloc[:,1:], train.life_exp_good)` be quite right?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q2\n",
    "\n",
    "testdata = test[[\"life_exp_good\", \"social_support\"]].dropna()\n",
    "Xt = testdata.iloc[:,1:]\n",
    "Yt = testdata.life_exp_good\n",
    "\n",
    "assert sum(Yt == clf.predict(Xt)) == 170, hint+\"\\n\\nState Check: \"+str(sum(Yt == clf.predict(Xt)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Now you can visualize your tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree.plot_tree(clf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Or to make it more immediately readible we can use graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=[\"social_support\"],\n",
    "                                class_names=[\"Bad\",\"Good\"],\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = gv.Source(dot_data, format='SVG')\n",
    "graph # Comment this line and re-run the cell so the figure doesn't render if your final file size is too large for MarkUs\n",
    "# Same solution if MarkUs notebook renderer gives you an error of \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "And to limit the visualization itself we can add the `max_depth` parameter to our call of `export_graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, max_depth=3,\n",
    "                                feature_names=[\"social_support\"],\n",
    "                                class_names=[\"Bad\",\"Good\"],\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = gv.Source(dot_data, format='SVG')\n",
    "graph # Comment this line and re-run the cell so the figure doesn't render if MarkUs notebook renderer gives you an error of \n",
    "# \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Q3: How many observation are in the training data set and the test data set?\n",
    "\n",
    "> - Hint: a single observation consists of all the measurements made on a single entity. In Machine Learning, the  \"vector\" of all values measured for a single entity comprise a single \"observation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q3_empty_cell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: your answer will be tested!\n",
    "num_train = None # Replace this with the number of observations in the training set\n",
    "num_test = None # Replace this with the number of observations in the test set\n",
    "Q3 = (num_train, num_test) # Do not change this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q3\n",
    "assert Q3 == (1038,260), \"You're using the `.shape` method, right?\"+\"\\n\\nState Check: \"+str(train.shape, test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Q4: Why did you fit the classification tree with the data set you did?\n",
    "\n",
    "#### Write a one to two sentence answer to this question in markdown cell below\n",
    "- Compare your response to the answer given in the ***MarkUs*** output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "> Answer here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3358dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += \"The training data set was used in order to save some test data which allows us to see how the model predicts on new data that it hasn't seen before and hasn't used to fit itself.  If we see how a model performs using data that it has had the chance to see and learn (and even potentially sort of memorize?) then we'll have a false sense of confidence in the capabilites of the model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q4\n",
    "assert False, hint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Q5: Comment on the complexity of the decision tree classification model visualized above, especially in light of the fact that only a single feature was used to predict the outcome in this model.\n",
    "\n",
    "\n",
    "#### Write a one to two sentence answer to this question in markdown cell below\n",
    "- Compare your response to the answer given in the ***MarkUs*** output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "> Answer here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29010c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += \"This decision tree seems like a really complex model with all sorts of branches as rules for making the prediction of good life expectancy based on just the single `social_support` variable. Since the `social_support` variable is a continuous numerical value, this means that predictions are being made based on very specific interval ranges of the values of this variable, which is highly suggestive of a very particular and specific model that could be at risk of overfitting to the specific idiosynchrosies of the given data set.  The training data set include 1038 observations; so, perhaps this might be enough to justify the complexity of the constructed decision tree; but, the way we could see if this was the case would be by seeing how the fitted decision tree performs on new data. I.e., can it generalize to new data that wasn't used in its construction? That would be very telling about how overfit it was to the idiosynchrosies of the data set used to fit it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q5\n",
    "assert False, hint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Q6: Use the `clf.predict()` method to answer the following questions and confirm your answer using the first `graphviz` visualization of the decision tree\n",
    "\n",
    "#### Use values of `False`  and `True` as given by `clf.predict()` (corresponding to \"Bad\" and \"Good\"): always be sure you know what binary/boolean predictions actually correspond to in the original terms of a problem\n",
    "\n",
    "a) Does your decision tree predict that a country with `social_support = 0.49` has good life expectancy?  \n",
    "b) what if `social_support = 0.5`  \n",
    "c) what if `social_support = 0.51`  \n",
    "d) what if `social_support = 0.9`  \n",
    "\n",
    "> - Hint: `pd.DataFrame({'name': [<values>]})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06c540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Q6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: your answer will be tested!\n",
    "Q6a = None # Replace this with True or False: don't supply something like array([False])\n",
    "Q6b = None\n",
    "Q6c = None\n",
    "Q6d = None\n",
    "Q6 = (Q6a, Q6b, Q6c, Q6d) # Do not change this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c35505",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Try google if you need help figuring out how to use `clf.predict()`; and,\\n\"\n",
    "hint += \"what happens if you pass `pd.DataFrame({'social_support': [.1,.2]})` to this method?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "test_Q6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Try google if you need help figuring out how to use `clf.predict()`; and,\nwhat happens if you pass `pd.DataFrame({'social_support': [.1,.2]})` to this method?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test_Q6\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Q6 \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m), hint\n",
      "\u001b[0;31mAssertionError\u001b[0m: Try google if you need help figuring out how to use `clf.predict()`; and,\nwhat happens if you pass `pd.DataFrame({'social_support': [.1,.2]})` to this method?"
     ]
    }
   ],
   "source": [
    "# test_Q6\n",
    "assert Q6 == (False, True, False, True), hint+\"\\n\\nAnswer Submitted: \"+str(Q6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Do these predictions make sense to you?\n",
    "\n",
    "Look at how these small differences in the input rapidly change the predicted label... it seems kind of strange. It's a little hard to intuitively see why predictions should change like this... it makes you wonder if the model is really doing anything meaningful here.  \n",
    "\n",
    "Perhaps the model might just actually be overly complex and convoluted and might be overinterpreting the data used to fit it (which we call overfitting). Since you'll probably agree that the behaviour of the model that we're observing seems a bit off, you'll probably also agree that it's a reasonable idea to try reduce the complexity of the model so it can be more reliably estimated with the data at hand.  With that in mind, create and fit a new classification tree `clf2` on the same inputs with a maximum depth of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clf2_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1985) # Do not change this line as there is some randomeness when calling `clf.fit(...)`\n",
    "clf2 = None # The parameter that you're looking for is `max_depth`\n",
    "# But note that this is fundamentally different than the `max_depth` parameter for `tree.export_graphviz` (why?)\n",
    "\n",
    "# Uncomment and run the line below to see the function documentation\n",
    "#tree.DecisionTreeClassifier?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Q7: Visualize the  `clf2` Decision Tree and reanswer the questions asked in Q6 with `clf2`.\n",
    "\n",
    "#### Use the same train/test split data used so far when fitting  `clf2` above.\n",
    "\n",
    "> - Hint: Make sure you're now using `clf2` rather than `clf` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your Decision Tree visualization code here: your code is what will be checked here\n",
    "\n",
    "graph # Comment this line and re-run the cell so the figure doesn't render if MarkUs notebook renderer gives you an error of \n",
    "# \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b92763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: your answer will be tested!\n",
    "Q7a = None # Replace this with True or False\n",
    "Q7b = None\n",
    "Q7c = None\n",
    "Q7d = None\n",
    "Q7 = (Q7a, Q7b, Q7c, Q7d) # Do not change this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q7_prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Try google if you need help figuring out how to use `clf.predict()`; and,\\n\"\n",
    "hint += \"what happens if you pass `pd.DataFrame({'social_support': [.1,.2]})` to this method?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q7\n",
    "assert Q7 == (False, False, False, True), hint+\"\\n\\nAnswer Submitted: \"+str(Q7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Train classification tree `clf3` with features `logGDP`, `social_support`, `freedom`, and `generosity` (to again predict if a country has good life expectancy)\n",
    "\n",
    "#### Use the same train/test split data used so far and use default (tuning) parameters (no maximum tree depth) when instantiating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clf3_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1985) # Do not change this line as there is some randomeness when calling `clf.fit(...)`\n",
    "clf3 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf3, out_file=None, max_depth=3,\n",
    "                                feature_names=[\"logGDP\", \"social_support\", \"freedom\", \"generosity\"],\n",
    "                                class_names=[\"Bad\", \"Good\"],\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = gv.Source(dot_data, format='SVG')\n",
    "graph # You can comment this line and re-run so the figure doesn't render if MarkUs notebook renderer gives you an error of \n",
    "# \"nbconvert failed: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\"\n",
    "# It's your responsibility to check this: TAs can't provide manual marks if your notebook doesn't render in MarkUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Q8: Use the testing dataset you created in Q1 to create confusion matrices for `clf2` and `clf3`. Report the sensitivity (true positive rate), specificity (true negative rate) and accuracy for each of the trees/models.\n",
    "\n",
    "#### Provide your answers as decimal numbers with three signifiant digits, such as `0.123` (and not as percentages like `12.3%`), and treat “Good” life expectancy as the positive response and prediction class. \n",
    "\n",
    "> - Hint 0: Use `np.round(0.1234,3)` to produce the correct rouding for the answers  \n",
    "> - Hint 1: Does the `y_true` or `y_pred` parameter go first in the `confusion_matrix` function?  \n",
    "> - Hint 2: Which columns/features of the `test` data set should be used for `clf2` versus `clf3`?  \n",
    "> - Hint 3: Making sure you know how a confusion matrix is labeled can be tricky; so, <br>to keep things\n",
    ">   easier for you the way it works is demonstrated below \n",
    ">   ```python\n",
    ">      ConfusionMatrixDisplay(confusion_matrix(..., labels=[False, True]), \n",
    ">                             display_labels=[\"Bad\",\"Good\"]).plot()\n",
    ">   ```\n",
    ">   (i.e., labels are sorted so `False` comes before `True` and this corresponds to \"Bad\" then \"Good\")\n",
    "> - Hint 4: If you need more help, see the [Confusion Matrices and Metrics](#cf) section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q8_empty_cell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Q8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: your answer will be tested!\n",
    "(clf2_sensitivity, clf2_specificity, clf2_accuracy) = (None, None, None) # Replace the `None`s with the corresponding answers\n",
    "(clf3_sensitivity, clf3_specificity, clf3_accuracy) = (None, None, None)\n",
    "Q8 = (clf2_sensitivity, clf2_specificity, clf2_accuracy) + (clf3_sensitivity, clf3_specificity, clf3_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb475c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Make sure you used `np.round(...,3), and didn't confuse what the rows, columns, and (TN, FN, FP, TP) cells of the confusion matrix are. Double check the (TN, FN, FP, TP based) definitions of sensitivity and specificity and accuracy (which can be found in the 'Confusion matrices and Metrics' section below). Confirm that your code matches the code below:\"\n",
    "hint += '''\\n\n",
    "ConfusionMatrixDisplay(confusion_matrix(test.life_exp_good, clf2.predict(test[['social_support']]), labels=[False, True]), display_labels=[\"Bad\",\"Good\"]).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix(test.life_exp_good, clf3.predict(test.iloc[:,1:])), display_labels=[\"Bad\",\"Good\"]).plot()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "test_Q8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf2_sensitivity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# test_Q8\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39massert\u001b[39;00m (clf2_sensitivity, clf2_specificity, clf2_accuracy, clf3_sensitivity, clf3_specificity, clf3_accuracy) \u001b[39m==\u001b[39m \\\n\u001b[0;32m      3\u001b[0m        (\u001b[39m0.716\u001b[39m,             \u001b[39m0.804\u001b[39m,           \u001b[39m0.769\u001b[39m,         \u001b[39m0.833\u001b[39m,             \u001b[39m0.886\u001b[39m,             \u001b[39m0.865\u001b[39m), hint\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAnswer Submitted: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(Q8)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf2_sensitivity' is not defined"
     ]
    }
   ],
   "source": [
    "# test_Q8\n",
    "assert (clf2_sensitivity, clf2_specificity, clf2_accuracy, clf3_sensitivity, clf3_specificity, clf3_accuracy) == \\\n",
    "       (0.716,             0.804,           0.769,         0.833,             0.886,             0.865), hint+\"\\n\\nAnswer Submitted: \"+str(Q8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Compared to understanding the contribution of different covariates towards the final predicted values of multiple linear regression models (where you can just read off the equation to see how predictions work), the extent to which we do not understand the overall contributions of the different features to the final predictions from our decision trees should feel a bit off-putting. To remedy this we can use so-called **Feature Importance** heuristics to judge how relatively important the different features are in the final decision tree predictions. There are a number of ways to do this (such as counting the number of nodes which split on each feature divided by the total number of splits in the tree), but as seen in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), a fit `DecisionTreeClassifier` model has a `.feature_importances_` attribute which measures the relative  contribution of each feature to the explanatory power of the model.  \n",
    "\n",
    "> The way a decision tree is fit is that at each step in the construction process of adding a new decision node splitting rule to the current tree structure. All possible decision rules for all possible variables are considered, and the one that improves the prediction the most (as measured by the criterion of either \"Gini impurity\" or \"Shannon entropy\") legally and sufficiently according to the tuning parameters rules of the decision tree is added to the decision tree.  The overall \"criterion\" noted above improves with each new decision node splitting rule, so the improvement can thus be tracked and the contributions attributed to the feature upon which the decision node splitting rule is based.  This means the relative contribution of each feature to the overall explanatory power of the model can be calculated, and this is what the `.feature_importances_` attribute does. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Q9: Which predictor variable is most important for making predictions according to clf3?\n",
    "\n",
    "> - Hint 0: The values in the `.feature_importances_` attribute correspond to values in the `.feature_names_in_` attribute\n",
    "> - Hint 1: The usual way to visualize Feature Importances would be as\n",
    "    ```python\n",
    "    import plotly.express as px\n",
    "    px.bar(pd.DataFrame({\"Feature Importances\": clf3.feature_importances_, \n",
    "                         \"Feature names\": clf3.feature_names_in_}), \n",
    "           x=\"Feature Importances\", y=\"Feature names\")    \n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9: your answer will be tested!\n",
    "Q9 = None # Replace this with the (str) name of the most relevant predictor variable\n",
    "# E.g., Q9 = \"A Variable Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q9\n",
    "assert Q9 == \"logGDP\", 'Try running `clf3.feature_importances_, clf3.feature_names_in_`'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Q10: Describe the differences and similarities of interpreting coefficients in linear model regression versus feature importances in decision trees.\n",
    "#### Write a 1-2 sentence answer to this question in markdown cell below\n",
    "- Compare your response to the answer given in the ***MarkUs*** output.\n",
    "\n",
    "> - Hint: Consider the differing goals of linear regression and classification trees."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "> Answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce55c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\n",
      "Included as an example answer for feedback purposes only\n",
      "\n",
      "Regression coefficients represent changes in average responses of the predicted value as the predictor variables change (by a single unit increase) but feature importances in decision trees just represent which features were more (relatively) relevant in constructing the decision tree for improved overall accuracy (as judged by some criterion like Gini or Entropy). As such, regression coefficients can be interpreted directly in terms of the scenario and can be readily compared across models, whereas feature importances are not interpreted the same way and instead only provide a relative ranking of relevance (for making more correct class predictions overall). To understand the particular impact of a specific feature, all the decision node splitting rules for the feature must be examined and their effects characterized.\n"
     ]
    }
   ],
   "source": [
    "hint = \"\\n\\nAUTOMATICALLY FAILING AUTOTEST: DOES NOT COUNT AGAINST STUDENT\\n\"\n",
    "hint += \"Included as an example answer for feedback purposes only\\n\\n\"\n",
    "hint += \"Regression coefficients represent changes in average responses of the predicted value as the predictor variables change (by a single unit increase) but feature importances in decision trees just represent which features were more (relatively) relevant in constructing the decision tree for improved overall accuracy (as judged by some criterion like Gini or Entropy). As such, regression coefficients can be interpreted directly in terms of the scenario and can be readily compared across models, whereas feature importances are not interpreted the same way and instead only provide a relative ranking of relevance (for making more correct class predictions overall). To understand the particular impact of a specific feature, all the decision node splitting rules for the feature must be examined and their effects characterized.\"\n",
    "print(hint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q10\n",
    "assert False, hint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "<a id='cf'></a>\n",
    "# Confusion Matrices and Metrics\n",
    "\n",
    "- **Accuracy** is the proportion of cases that are correctly identified.\n",
    "- **Sensitivity** is the proportion of actual positive cases which are correctly identified to be positive (as true positives)\n",
    "    - **Sensitivity** is also known as **true positive rate (TPR)**\n",
    "- **Specificity** is the proportion of actual negative cases which are correctly identified to be negative (as true negative)\n",
    "    - **Specificity** is also known as **true negative rate (TNR)**\n",
    "- **False positive rates (FPR)** are defined to be the proportion of actually negative cases which are incorrectly identified (as false positives)\n",
    "- **False negative rates (FNR)** are defined to be the proportion of actually positive cases which are incorrectly identified (as false negatives)\n",
    "    - *but noticed how the FPR and FNR work in a sort of \"flipped\" manner in these definitions as they are defined with respect to the truth*\n",
    "\n",
    "In formulas\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy}  & = {} (TP+TN)/\\text{\"total # of cases\"}\\\\\n",
    "TPR & = {} TP/(TP+FN) = 1-FNR \\\\\n",
    "TNR & = {} TN/(TN+FP) = 1-FPR\n",
    "\\end{align*}\n",
    "\n",
    "and you can read more and see a (greatly expanded) handy list of formulas at the following [wikipedia page.](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Two classification trees were built to predict which individuals have a disease using different sets of potential predictors. We use each of these trees to predict disease status for 100 new individuals. Below are confusion matrices corresponding to these two classification trees. The columns are the actual outcome, the rows are predicted outcomes (which is different than how the `ConfusionMatrixDisplay(confusion_matrix(...` functions worked about... you always have to make sure you know what the labels on your confusion matrices are!!)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Tree A**         | Disease | No disease | $\\hspace{1in}$ | **Tree B**         | Disease | No disease |\n",
    "|--------------------|---------|------------|----------------|--------------------|---------|------------|\n",
    "| Predict disease    | 36      | 22         |                | Predict disease    | 24      | 6          |\n",
    "| Predict no disease | 2       | 40         |                | Predict no disease | 14      | 56         |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Q11: Calculate the accuracy, false-positive rate, and false negative rate for each classification tree.\n",
    "Here, a “positive” result means we predict an individual has the disease and a “negative” result means we predict they do not.\n",
    "Round each value to 2 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "Q11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11: your answer will be tested!\n",
    "TreeA_accuracy = None\n",
    "TreeA_false_positive_rate = None\n",
    "TreeA_false_negative_rate = None\n",
    "\n",
    "TreeB_accuracy = None\n",
    "TreeB_false_positive_rate = None\n",
    "TreeB_false_negative_rate = None\n",
    "\n",
    "Q11 = (TreeA_accuracy, TreeA_false_positive_rate, TreeA_false_negative_rate, TreeB_accuracy, TreeB_false_positive_rate, TreeB_false_negative_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab7f4498",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"Make sure you round correctly; calculate the FALSE positive and negative rates; and aren't mixing up your rows (predicted values) and your columns (actual values) in your relative ratios!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "test_Q11",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Make sure you round correctly; calculate the FALSE positive and negative rates; and aren't mixing up your rows (predicted values) and your columns (actual values) in your relative ratios!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test_Q11\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (TreeA_accuracy, TreeA_false_positive_rate, TreeA_false_negative_rate, TreeB_accuracy, TreeB_false_positive_rate, TreeB_false_negative_rate) \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m.76\u001b[39m,\u001b[38;5;241m.35\u001b[39m,\u001b[38;5;241m.05\u001b[39m,\u001b[38;5;241m.80\u001b[39m,\u001b[38;5;241m.10\u001b[39m,\u001b[38;5;241m.37\u001b[39m), hint\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure you round correctly; calculate the FALSE positive and negative rates; and aren't mixing up your rows (predicted values) and your columns (actual values) in your relative ratios!"
     ]
    }
   ],
   "source": [
    "# test_Q11\n",
    "\n",
    "#np.round((36+40)/100,2)\n",
    "#np.round(1 - 40/(40+22),2)\n",
    "#np.round(1 - 36/(36+2),2)\n",
    "#np.round((24+56)/100,2)\n",
    "#np.round(1 - 56/(56+6),2)\n",
    "#np.round(1 - 24/(24+14),2)\n",
    "\n",
    "assert (TreeA_accuracy, TreeA_false_positive_rate, TreeA_false_negative_rate, TreeB_accuracy, TreeB_false_positive_rate, TreeB_false_negative_rate) == (.76,.35,.05,.80,.10,.37), hint+\"\\n\\nAnswer Submitted: \"+str(Q11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Q12: Which tree would you prefer to put into use in predicting if individuals are ill?\n",
    "#### Write a 1-3 sentence answer to this question in markdown cell below.\n",
    "\n",
    "> - Hint 1: Make reference to the metrics you calculated in Q11, and any others you think might matter.\n",
    "> - Hint 2: Interpret what the metrics mean in the context of the problem before deciding how much the metrics matter to you.\n",
    "> - Hint 3: Indicate what tradeoffs might you find acceptable and unacceptable.\n",
    "\n",
    "- This question will be manually graded by TAs. They are looking for a well-written and reasonably sensible answer as opposed to a specific answer.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "> Answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "test_Q12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You might prefer tree A to tree B because it has a lower false negative rate, meaning that more people who need treatment will get it. Alternatively you could argue that tree B is better because it has a higher overall accuracy and depending on the disease, a false positive might be more damaging than a false negative.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed_test_Q12\n",
    "# assert False, \n",
    "\"You might prefer tree A to tree B because it has a lower false negative rate, meaning that more people who need treatment will get it. Alternatively you could argue that tree B is better because it has a higher overall accuracy and depending on the disease, a false positive might be more damaging than a false negative.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Geometric Interpretation of Decision Tree Predictions\n",
    "Data was collected on 30 cancer patients to investigate the effectiveness (Yes/No) of a treatment. Two quantitative variables x1 and x2 (taking values between 0 and 1) are thought to be important predictors of effectiveness. Suppose that the rectangles labeled as nodes in the scatter plot below represent nodes of a classification tree.\n",
    "![Scatter plot with a horizontal x1 axis and vertical x2 axis, both ranging from 0.00 to 1.00, and blue triangular points representing Effectiveness = 'Yes' and round orange points representing Effectiveness = 'No'. It is divided into 4 regions, labelled nodes 1-4. Node 1 is the bottom left region, node 4 the bottom right, node 3 the top left, and node 2 the top right. The top regions are divided from the bottom ones by a horizontal line along x_2=0.50. Node 2 is separated from node 3 by a line at x1=0.50. Node 2 is separated from node 3 by a line at x1=0.50. Node 1 has 5 'Yes' nodes and 7 'No' Nodes. Node 2 has 3 'Yes' nodes and 2 'No' Nodes. Node 3 has 1 'Yes' node and 3 'No' Nodes. Node 4 has 2 'Yes' nodes and 7 'No' Nodes.](im/9/HW9_Q7_Graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Q13: The diagram above is the geometric interpretation of a classification tree to predict drug effectiveness based on two predictors, x1 and x2. What is the predicted class of each node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "Q13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q13: your answer will be tested!\n",
    "Q13_Node1 = None # Replace with 'Yes' or 'No'\n",
    "Q13_Node2 = None\n",
    "Q13_Node3 = None\n",
    "Q13_Node4 = None\n",
    "Q13 = (Q13_Node1, Q13_Node2, Q13_Node3, Q13_Node4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "560e67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hint = \"The classification tree will pick 'Yes' or 'No' for each region depending on which is more common in that region.\"\n",
    "test = (Q13_Node1, Q13_Node2, Q13_Node3, Q13_Node4) == ('No', 'Yes', 'No', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "test_Q13",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The classification tree will pick 'Yes' or 'No' for each region depending on which is more common in that region.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test_Q13\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test, hint\n",
      "\u001b[0;31mAssertionError\u001b[0m: The classification tree will pick 'Yes' or 'No' for each region depending on which is more common in that region."
     ]
    }
   ],
   "source": [
    "# test_Q13\n",
    "assert test, hint+\"\\n\\nAnswer Submitted: \"+str(Q13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Q14: What is the first variable that the decision tree splits on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q14: your answer will be tested!\n",
    "Q14 = None # Replace with 'x1' or 'x2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_Q14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Q14\n",
    "assert Q14 == 'x2', \"Notice how the lines in the diagram correspond to splits in the decision tree\"+\"\\n\\nAnswer Submitted: \"+str(Q14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
